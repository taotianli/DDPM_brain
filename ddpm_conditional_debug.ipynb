{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from utils import *\n",
    "from modules import UNet_conditional, EMA, UNet_conditional_concat, UNet_conditional_fully_concat, UNet_conditional_fully_add\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", level=logging.INFO, datefmt=\"%I:%M:%S\")\n",
    "\"\"\"\n",
    "需要修改的地方：\n",
    "1:把label换成image encoder，后续可能换成transformer\n",
    "5:改进guidance的方式(corss attention)\n",
    "7:加上实际的评价指标\n",
    "11:修改部分\n",
    "14:每100个epoch 一次所有数据\n",
    "15:使用相邻slice做guidance\n",
    "16:sample之后重新变成图片这一步可能有问题，不是直接归一化，可以用其他操作\n",
    "17:sample之后加一个inpaint，输出图像\n",
    "18:sample之后计算指标\n",
    "19:\n",
    "\"\"\"\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=240, device=\"cuda\"):\n",
    "        self.noise_steps = noise_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "\n",
    "        self.beta = self.prepare_noise_schedule().to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.device = device\n",
    "\n",
    "    def prepare_noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
    "\n",
    "    def noise_images(self, x, t):\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
    "        Ɛ = torch.randn_like(x)\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * Ɛ, Ɛ\n",
    "\n",
    "    def _timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
    "\n",
    "    def sample(self, model, n, labels, cfg_scale=0):\n",
    "        logging.info(f\"Sampling {n} new images....\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn((n, 1, self.img_size, self.img_size)).to(self.device)\n",
    "            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
    "                t = (torch.ones(n) * i).long().to(self.device)\n",
    "                predicted_noise = model(x, t, labels)\n",
    "                if cfg_scale > 0:\n",
    "                    uncond_predicted_noise = model(x, t, None)\n",
    "                    predicted_noise = torch.lerp(uncond_predicted_noise, predicted_noise, cfg_scale)\n",
    "                alpha = self.alpha[t][:, None, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
    "                beta = self.beta[t][:, None, None, None]\n",
    "                if i > 1:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x)\n",
    "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "        # model.train()\n",
    "        # x = (x.clamp(-1, 1) + 1) / 2\n",
    "        # x = (x * 255).type(torch.uint8)\n",
    "        return x\n",
    "\n",
    "def inpaint_image(original_image, generated_image, mask):\n",
    "    \"\"\"\n",
    "    将生成的图像融合到原始图像的指定区域中。\n",
    "    \n",
    "    参数:\n",
    "    original_image (torch.Tensor): 原始图像\n",
    "    generated_image (torch.Tensor): 生成的图像\n",
    "    mask (torch.Tensor): 掩码图像, 1表示需要inpaint的区域, 0表示保留原图\n",
    "    \n",
    "    返回:\n",
    "    torch.Tensor: 输出的合成图像\n",
    "    \"\"\"\n",
    "    # 将三个输入tensor转换到相同的设备上\n",
    "    device = original_image.device\n",
    "    mask = mask.to(device)\n",
    "    generated_image = generated_image.to(device)\n",
    "    \n",
    "    # 使用掩码融合原图和生成的图像\n",
    "    output_image = original_image.clone()\n",
    "    # print(output_image.shape, mask.shape, generated_image.shape)\n",
    "    output_image = output_image * (1 - mask) + generated_image * mask\n",
    "    \n",
    "    return output_image\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    setup_logging(args.run_name)\n",
    "    device = args.device\n",
    "    dataloader = get_data(args)\n",
    "    test_dataloader = get_test_data(args)\n",
    "    model = UNet_conditional_fully_add().to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
    "    mse = nn.MSELoss()\n",
    "    diffusion = Diffusion(img_size=args.image_size, device=device)\n",
    "    logger = SummaryWriter(os.path.join(\"runs\", args.run_name))\n",
    "    l = len(dataloader)\n",
    "    ema = EMA(0.995)\n",
    "    ema_model = copy.deepcopy(model).eval().requires_grad_(False)\n",
    "\n",
    "    # for epoch in range(args.epochs):\n",
    "    #     logging.info(f\"Starting epoch {epoch}:\")\n",
    "    #     pbar = tqdm(dataloader)\n",
    "    #     for i, (images, cropped_images, masks) in enumerate(pbar):\n",
    "    #         # print(images.shape)\n",
    "    #         # images = images.to(device)\n",
    "    #         labels = cropped_images\n",
    "    #         b, c, l, w = images.shape\n",
    "\n",
    "    #         images_predict = torch.zeros_like(images)\n",
    "    #         noise_predict = torch.zeros_like(images)\n",
    "\n",
    "    #         # print(slice) #size 应该是 b, 1, 240, 240\n",
    "    #         images_slice = images[:,:,:,:]\n",
    "    #         labels_slice = labels[:,:,:,:]\n",
    "    #         #去掉一个维度\n",
    "\n",
    "    #         images_slice = images_slice.to(device)\n",
    "    #         labels_slice = labels_slice.to(device)\n",
    "    #         images_slice = images_slice.to(torch.float)\n",
    "    #         labels_slice = labels_slice.to(torch.float)\n",
    "    #         # print('input shape', images_slice.shape)\n",
    "\n",
    "\n",
    "    #         t = diffusion._timesteps(images_slice.shape[0]).to(device)\n",
    "    #         x_t, noise = diffusion.noise_images(images_slice, t)\n",
    "    #         predicted_noise = model(x_t, t, labels_slice)\n",
    "    #         images_predict[:,:,:,:] = predicted_noise\n",
    "    #         noise_predict[:,:,:,:] = noise\n",
    "    #         images_predict_slice = inpaint_image(images[:,:,:,:], images_predict[:,:,:,:], masks[:,:,:,:])\n",
    "    #         noise_predict_slice = inpaint_image(images[:,:,:,:], noise_predict[:,:,:,:], masks[:,:,:,:])\n",
    "    #         loss = mse(noise_predict_slice, images_predict_slice)\n",
    "\n",
    "    #         optimizer.zero_grad()\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #         ema.step_ema(ema_model, model)\n",
    "\n",
    "    #         pbar.set_postfix(MSE=loss.item())\n",
    "    #         logger.add_scalar(\"MSE\", loss.item(), global_step=epoch * l + i)\n",
    "\n",
    "        \n",
    "        # images, cropped_images, masks = next(iter(pbar))\n",
    "        # b, _, _, _ = images.shape\n",
    "        # print('batch size:', b)\n",
    "        # d_images = diffusion.(model, n=b, labels=cropped_images)\n",
    "        # print(d_images.shape)\n",
    "        # # ema_d_images = diffusion.(ema_model, n=b, labels=cropped_images)\n",
    "        # # plot_images(d_images)\n",
    "        # save_images(d_images, os.path.join(\"results\", args.run_name, f\"{epoch}.jpg\"))\n",
    "        # # save_images(ema_d_images, os.path.join(\"results\", args.run_name, f\"{epoch}_ema.jpg\"))\n",
    "        # torch.save(model.state_dict(), os.path.join(\"models\", args.run_name, f\"ckpt.pt\"))\n",
    "        # torch.save(ema_model.state_dict(), os.path.join(\"models\", args.run_name, f\"ema_ckpt.pt\"))\n",
    "        # torch.save(optimizer.state_dict(), os.path.join(\"models\", args.run_name, f\"optim.pt\"))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "args, unknown = parser.parse_known_args()\n",
    "args.run_name = \"DDPM_conditional\"\n",
    "args.batch_size = 5\n",
    "args.image_size = 96#这个参数有什么用，和原始图像的关系是什么\n",
    "# args.dataset_path =  r\"D:\\ASNR-MICCAI-BraTS2023-Local-Synthesis-Challenge-Training\"\n",
    "args.dataset_path =  r\"C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\"\n",
    "args.device = \"cuda\"\n",
    "args.lr = 3e-4\n",
    "args.train = True\n",
    "args.shuffle = False\n",
    "device = 'cuda'\n",
    "dataloader = get_data(args)\n",
    "model = UNet_conditional_fully_add().to(device)\n",
    "ckpt = torch.load(\"./models/DDPM_conditional/ema_ckpt.pt\")\n",
    "model.load_state_dict(ckpt)\n",
    "diffusion = Diffusion(img_size=args.image_size, device=device)\n",
    "pbar = tqdm(dataloader)\n",
    "images, cropped_images, masks = next(iter(pbar))\n",
    "b, _, _, _ = images.shape\n",
    "d_images = diffusion.sample(model, n=b, labels=cropped_images)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inpaint_image(original_image, generated_image, mask):\n",
    "    \"\"\"\n",
    "    将生成的图像融合到原始图像的指定区域中。\n",
    "    \n",
    "    参数:\n",
    "    original_image (torch.Tensor): 原始图像\n",
    "    generated_image (torch.Tensor): 生成的图像\n",
    "    mask (torch.Tensor): 掩码图像, 1表示需要inpaint的区域, 0表示保留原图\n",
    "    \n",
    "    返回:\n",
    "    torch.Tensor: 输出的合成图像\n",
    "    \"\"\"\n",
    "    # 将三个输入tensor转换到相同的设备上\n",
    "    device = original_image.device\n",
    "    mask = mask.to(device)\n",
    "    generated_image = generated_image.to(device)\n",
    "    \n",
    "    # 使用掩码融合原图和生成的图像\n",
    "    output_image = original_image.clone()\n",
    "    print(output_image.shape, mask.shape, generated_image.shape)\n",
    "    # print(output_image.shape, mask.shape, generated_image.shape)\n",
    "    reference_image = output_image * mask\n",
    "    output_image = output_image * (1 - mask) + generated_image * mask\n",
    "    generated_image = generated_image * mask\n",
    "    \n",
    "    return output_image, generated_image, reference_image\n",
    "images_predict_slice, generated_image, reference_image = inpaint_image(images[:,:,:,:], d_images[:,:,:,:], masks[:,:,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# 选择要显示的图像索引\n",
    "index = 4\n",
    "images = images.cpu()\n",
    "d_images_new = images_predict_slice.cpu()\n",
    "dd = d_images[:,:,:,:].cpu()\n",
    "cropped_images = cropped_images.cpu()\n",
    "g_images = generated_image.cpu()\n",
    "r_images = reference_image.cpu()\n",
    "masks_clone = masks.clone().cpu()\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "# 获取选中的图像\n",
    "img = images[index, 0, :, :]\n",
    "d_img = d_images_new[index, 0, :, :]\n",
    "c_img = cropped_images[index, 0, :, :]\n",
    "g_img = g_images[index, 0, :, :]\n",
    "r_img = r_images[index, 0, :, :]\n",
    "dd_img = dd[index, 0, :, :]\n",
    "# print(g_img)\n",
    "# print(img[10,:])\n",
    "if torch.any(g_img != 0):\n",
    "    print(\"g_img 中包含非0元素\")\n",
    "else:\n",
    "    print(\"g_img 中全是0\")\n",
    "non_zero_elements = g_img[g_img != 0]\n",
    "# print(non_zero_elements)\n",
    "# 使用 matplotlib 显示图像\n",
    "plt.figure(figsize=(8, 8))\n",
    "axes[0,0].imshow(img, cmap='gray')\n",
    "axes[0,1].imshow(d_img, cmap='gray')\n",
    "axes[1,0].imshow(dd_img, cmap='gray')\n",
    "axes[1,1].imshow(g_img, cmap='gray')\n",
    "plt.show()\n",
    "mse = nn.MSELoss()\n",
    "# loss = mse(reference_image, masks_clone)\n",
    "# print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
