{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from utils import *\n",
    "from modules import UNet_conditional, EMA, UNet_conditional_concat, UNet_conditional_fully_concat, UNet_conditional_fully_add, UNet_conditional_concat_with_mask, UNet_conditional_concat_with_mask_v2\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", level=logging.INFO, datefmt=\"%I:%M:%S\")\n",
    "\"\"\"\n",
    "需要修改的地方：\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=240, device=\"cuda\"):\n",
    "        self.noise_steps = noise_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "\n",
    "        self.beta = self.prepare_noise_schedule().to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.device = device\n",
    "\n",
    "    def prepare_noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
    "\n",
    "    def noise_images(self, x, t):\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
    "        Ɛ = torch.randn_like(x)\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * Ɛ, Ɛ\n",
    "\n",
    "    def _timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
    "\n",
    "    def sample(self, model, n, labels, masks, cfg_scale=0):\n",
    "        logging.info(f\"Sampling {n} new images....\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn((n, 1, self.img_size, self.img_size)).to(self.device)\n",
    "            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
    "                t = (torch.ones(n) * i).long().to(self.device)\n",
    "                # predicted_noise = model(x, t, labels)\n",
    "                predicted_noise = model(x, t, labels, masks)\n",
    "                if cfg_scale > 0:\n",
    "                    uncond_predicted_noise = model(x, t, None)\n",
    "                    predicted_noise = torch.lerp(uncond_predicted_noise, predicted_noise, cfg_scale)\n",
    "                alpha = self.alpha[t][:, None, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
    "                beta = self.beta[t][:, None, None, None]\n",
    "                if i > 1:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x)\n",
    "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "        return x\n",
    "\n",
    "def inpaint_image(original_image, generated_image, mask):\n",
    "    \"\"\"\n",
    "    将生成的图像融合到原始图像的指定区域中。\n",
    "    \n",
    "    参数:\n",
    "    original_image (torch.Tensor): 原始图像\n",
    "    generated_image (torch.Tensor): 生成的图像\n",
    "    mask (torch.Tensor): 掩码图像, 1表示需要inpaint的区域, 0表示保留原图\n",
    "    \n",
    "    返回:\n",
    "    torch.Tensor: 输出的合成图像\n",
    "    \"\"\"\n",
    "    # 将三个输入tensor转换到相同的设备上\n",
    "    device = original_image.device\n",
    "    mask = mask.to(device)\n",
    "    generated_image = generated_image.to(device)\n",
    "    \n",
    "    # 使用掩码融合原图和生成的图像\n",
    "    output_image = original_image.clone()\n",
    "    # print(output_image.shape, mask.shape, generated_image.shape)\n",
    "    output_image = output_image * (1 - mask) + generated_image * mask\n",
    "    \n",
    "    return output_image\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "args, unknown = parser.parse_known_args()\n",
    "args.run_name = \"DDPM_conditional\"\n",
    "args.batch_size = 10\n",
    "args.image_size = 96#这个参数有什么用，和原始图像的关系是什么\n",
    "# args.dataset_path =  r\"D:\\ASNR-MICCAI-BraTS2023-Local-Synthesis-Challenge-Training\"\n",
    "args.dataset_path =  r\"C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\"\n",
    "args.device = \"cuda\"\n",
    "args.lr = 3e-4\n",
    "args.train = True\n",
    "args.shuffle = False\n",
    "device = 'cuda'\n",
    "dataloader = get_data(args)\n",
    "model = UNet_conditional_concat_with_mask_v2().to(device)\n",
    "# model = UNet_conditional_concat().to(device)\n",
    "ckpt = torch.load(\"./models/DDPM_conditional/ema_ckpt.pt\")\n",
    "model.load_state_dict(ckpt)\n",
    "diffusion = Diffusion(img_size=args.image_size, device=device)\n",
    "pbar = tqdm(dataloader)\n",
    "images, cropped_images, masks = next(iter(pbar))\n",
    "modefied_images = cropped_images\n",
    "b, _, _, _ = images.shape\n",
    "d_images = diffusion.sample(model, n=b, labels=modefied_images, masks=masks)\n",
    "\n",
    "# d_images = diffusion.sample(model, n=b, labels=modefied_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inpaint_image(original_image, generated_image, mask):\n",
    "    \"\"\"\n",
    "    将生成的图像融合到原始图像的指定区域中。\n",
    "    \n",
    "    参数:\n",
    "    original_image (torch.Tensor): 原始图像\n",
    "    generated_image (torch.Tensor): 生成的图像\n",
    "    mask (torch.Tensor): 掩码图像, 1表示需要inpaint的区域, 0表示保留原图\n",
    "    \n",
    "    返回:\n",
    "    torch.Tensor: 输出的合成图像\n",
    "    \"\"\"\n",
    "    # 将三个输入tensor转换到相同的设备上\n",
    "    device = original_image.device\n",
    "    mask = mask.to(device)\n",
    "    generated_image = generated_image.to(device)\n",
    "    \n",
    "    # 使用掩码融合原图和生成的图像\n",
    "    output_image = original_image.clone()\n",
    "    print(output_image.shape, mask.shape, generated_image.shape)\n",
    "    # print(output_image.shape, mask.shape, generated_image.shape)\n",
    "    reference_image = output_image * mask\n",
    "    output_image = output_image * (1 - mask) + generated_image * mask\n",
    "    generated_image = generated_image * mask\n",
    "    \n",
    "    return output_image, generated_image, reference_image\n",
    "images_predict_slice, generated_image, reference_image = inpaint_image(images[:,:,:,:], d_images[:,:,:,:], masks[:,:,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# 选择要显示的图像索引\n",
    "index = 8\n",
    "images = images.cpu()\n",
    "d_images_new = images_predict_slice.cpu()\n",
    "dd = d_images[:,:,:,:].cpu()\n",
    "cropped_images = modefied_images.cpu()\n",
    "g_images = generated_image.cpu()\n",
    "r_images = reference_image.cpu()\n",
    "masks_clone = masks.cpu()\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "# 获取选中的图像\n",
    "img = images[index, 0, :, :]\n",
    "d_img = d_images_new[index, 0, :, :]\n",
    "c_img = cropped_images[index, 0, :, :]\n",
    "g_img = g_images[index, 0, :, :]\n",
    "r_img = r_images[index, 0, :, :]\n",
    "dd_img = dd[index, 0, :, :]\n",
    "mask = masks_clone[index, 0, :, :]\n",
    "# print(g_img)\n",
    "# print(img[10,:])\n",
    "if torch.any(g_img != 0):\n",
    "    print(\"g_img 中包含非0元素\")\n",
    "else:\n",
    "    print(\"g_img 中全是0\")\n",
    "non_zero_elements = g_img[g_img != 0]\n",
    "# print(non_zero_elements)\n",
    "# 使用 matplotlib 显示图像\n",
    "plt.figure(figsize=(8, 8))\n",
    "axes[0,0].imshow(img, cmap='gray')\n",
    "axes[0,0].set_title('Ground-truth')\n",
    "axes[0,1].imshow(c_img, cmap='gray')\n",
    "axes[0,1].set_title('Cropped guidance')\n",
    "axes[1,0].imshow(dd_img, cmap='gray')\n",
    "axes[1,0].set_title('DDPM genarated')\n",
    "axes[1,1].imshow(d_img, cmap='gray')\n",
    "axes[1,1].set_title('Final infilled image')\n",
    "plt.show()\n",
    "mse = nn.MSELoss()\n",
    "# loss = mse(reference_image, masks_clone)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_1 = d_images_new[1, 0, :, :]\n",
    "d_2 = d_images_new[2, 0, :, :]\n",
    "d_3 = d_images_new[3, 0, :, :]\n",
    "d_4 = d_images_new[4, 0, :, :]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "plt.figure(figsize=(8, 8))\n",
    "axes[0,0].imshow(d_1, cmap='gray')\n",
    "axes[0,0].set_title('Slice 1')\n",
    "axes[0,1].imshow(d_2, cmap='gray')\n",
    "axes[0,1].set_title('Slice 2')\n",
    "axes[1,0].imshow(d_3, cmap='gray')\n",
    "axes[1,0].set_title('Slice 3')\n",
    "axes[1,1].imshow(d_4, cmap='gray')\n",
    "axes[1,1].set_title('Slice 4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mean_squared_error(\n",
    "    target: torch.Tensor,\n",
    "    prediction: torch.Tensor,\n",
    "    squared: bool = True,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the mean squared error between the target and prediction.\n",
    "\n",
    "    Args:\n",
    "        target (torch.Tensor): The target tensor.\n",
    "        prediction (torch.Tensor): The prediction tensor.\n",
    "        TODO update documentation\n",
    "    \"\"\"\n",
    "    mse = MeanSquaredError(\n",
    "        squared=squared,\n",
    "    )\n",
    "\n",
    "    return mse(preds=prediction, target=target)\n",
    "\n",
    "\n",
    "def _peak_signal_noise_ratio(\n",
    "    target: torch.Tensor,\n",
    "    prediction: torch.Tensor,\n",
    "    data_range: tuple = None,\n",
    "    epsilon: float = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the peak signal to noise ratio between the target and prediction.\n",
    "\n",
    "    Args:\n",
    "        target (torch.Tensor): The target tensor.\n",
    "        prediction (torch.Tensor): The prediction tensor.\n",
    "        data_range (tuple, optional): If not None, this data range (min, max) is used as enumerator instead of computing it from the given data. Defaults to None.\n",
    "        epsilon (float, optional): If not None, this epsilon is added to the denominator of the fraction to avoid infinity as output. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    if epsilon == None:\n",
    "        psnr = (\n",
    "            PeakSignalNoiseRatio()\n",
    "            if data_range == None\n",
    "            else PeakSignalNoiseRatio(data_range=data_range[1] - data_range[0])\n",
    "        )\n",
    "        return psnr(preds=prediction, target=target)\n",
    "    else:  # implementation of PSNR that does not give 'inf'/'nan' when 'mse==0'\n",
    "        mse = _mean_squared_error(target=target, prediction=prediction)\n",
    "        if data_range == None:  # compute data_range like torchmetrics if not given\n",
    "            min_v = (\n",
    "                0 if torch.min(target) > 0 else torch.min(target)\n",
    "            )  # look at this line\n",
    "            max_v = torch.max(target)\n",
    "        else:\n",
    "            min_v, max_v = data_range\n",
    "        return 10.0 * torch.log10(((max_v - min_v) ** 2) / (mse + epsilon))\n",
    "\n",
    "\n",
    "def _mean_squared_log_error(\n",
    "    target: torch.Tensor,\n",
    "    prediction: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the mean squared log error between the target and prediction.\n",
    "\n",
    "    Args:\n",
    "        target (torch.Tensor): The target tensor.\n",
    "        prediction (torch.Tensor): The prediction tensor.\n",
    "    \"\"\"\n",
    "    mle = MeanSquaredLogError()\n",
    "    return mle(preds=prediction, target=target)\n",
    "\n",
    "\n",
    "def _mean_absolute_error(\n",
    "    target: torch.Tensor,\n",
    "    prediction: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the mean absolute error between the target and prediction.\n",
    "\n",
    "    Args:\n",
    "        target (torch.Tensor): The target tensor.\n",
    "        prediction (torch.Tensor): The prediction tensor.\n",
    "    \"\"\"\n",
    "    mae = MeanAbsoluteError()\n",
    "    return mae(preds=prediction, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n",
    "from torchmetrics.regression import (\n",
    "    MeanAbsoluteError,\n",
    "    MeanSquaredError,\n",
    "    MeanSquaredLogError,\n",
    ")\n",
    "\n",
    "def _structural_similarity_index(\n",
    "    target: torch.Tensor,\n",
    "    prediction: torch.Tensor,\n",
    "    mask: torch.Tensor = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the structural similarity index between the target and prediction.\n",
    "\n",
    "    Args:\n",
    "        target (torch.Tensor): The target tensor.\n",
    "        prediction (torch.Tensor): The prediction tensor.\n",
    "        mask (torch.Tensor, optional): The mask tensor. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The structural similarity index.\n",
    "    \"\"\"\n",
    "    ssim = StructuralSimilarityIndexMeasure(return_full_image=True)\n",
    "    _, ssim_idx_full_image = ssim(preds=prediction, target=target)\n",
    "    mask = torch.ones_like(ssim_idx_full_image) if mask is None else mask\n",
    "    # ssim_idx = None\n",
    "    # mask = mask.bool()\n",
    "    # print(mask.bool())\n",
    "    # print(ssim_idx_full_image.shape)\n",
    "    try:\n",
    "        ssim_idx = ssim_idx_full_image[mask]\n",
    "        # print('here')\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        if len(ssim_idx_full_image.shape) == 0:\n",
    "            ssim_idx = torch.ones_like(mask) * ssim_idx_full_image\n",
    "    return ssim_idx.mean()\n",
    "\n",
    "\n",
    "def image_preprocess(tensor):\n",
    "    # 首先将tensor转换为numpy数组\n",
    "    image = tensor.numpy()\n",
    "    \n",
    "    # 对numpy数组进行clip和归一化操作\n",
    "    t1_clipped = np.clip(image, np.quantile(image, 0.001), np.quantile(image, 0.999))\n",
    "    t1_normalized = (t1_clipped - np.min(t1_clipped)) / (np.max(t1_clipped) - np.min(t1_clipped))\n",
    "    \n",
    "    # 将归一化后的numpy数组转换回tensor\n",
    "    normalized_tensor = torch.from_numpy(t1_normalized)\n",
    "    \n",
    "    return normalized_tensor\n",
    "\n",
    "\n",
    "img = image_preprocess(img)\n",
    "d_img = image_preprocess(d_img)\n",
    "print(img.shape, d_img.shape)\n",
    "img_copy = img.unsqueeze(0).unsqueeze(0)\n",
    "d_img_copy = d_img.unsqueeze(0).unsqueeze(0)\n",
    "mask_copy = mask.unsqueeze(0).unsqueeze(0)\n",
    "mask_copy = mask_copy.bool()\n",
    "\n",
    "\n",
    "ssim = _structural_similarity_index(\n",
    "        target=img_copy,\n",
    "        prediction=d_img_copy,\n",
    "        mask=mask_copy,\n",
    "    ).item()\n",
    "mse = _mean_squared_error(\n",
    "        target=img_copy,\n",
    "        prediction=d_img_copy,\n",
    "        squared=True,\n",
    "    ).item()\n",
    "psnr = _peak_signal_noise_ratio(\n",
    "        target=img_copy,\n",
    "        prediction=d_img_copy,\n",
    "    ).item()\n",
    "print(ssim, mse, psnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from utils import *\n",
    "# from utils import get_data_inference\n",
    "from modules import UNet_conditional, EMA, UNet_conditional_concat, UNet_conditional_fully_concat, UNet_conditional_fully_add\n",
    "from modules import UNet_conditional_concat_with_mask, UNet_conditional_concat_with_mask_v2, UNet_conditional_concat_Large\n",
    "from modules import UNet_conditional_concat_XLarge\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from utils import _structural_similarity_index, _peak_signal_noise_ratio, _mean_squared_error\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", level=logging.INFO, datefmt=\"%I:%M:%S\")\n",
    "\"\"\"\n",
    "需要修改的地方：\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=240, device=\"cuda\"):\n",
    "        self.noise_steps = noise_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "\n",
    "        self.beta = self.prepare_noise_schedule().to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.device = device\n",
    "\n",
    "    def prepare_noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
    "\n",
    "    def noise_images(self, x, t):\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
    "        Ɛ = torch.randn_like(x)\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * Ɛ, Ɛ\n",
    "\n",
    "    def _timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
    "\n",
    "    def sample(self, model, n, labels, masks, cfg_scale=0):\n",
    "        logging.info(f\"Sampling {n} new images....\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn((n, 1, self.img_size, self.img_size)).to(self.device)\n",
    "            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
    "                t = (torch.ones(n) * i).long().to(self.device)\n",
    "                # predicted_noise = model(x, t, labels)\n",
    "                print(labels.shape, masks.shape)\n",
    "                predicted_noise = model(x, t, labels, masks)\n",
    "                if cfg_scale > 0:\n",
    "                    uncond_predicted_noise = model(x, t, None)\n",
    "                    predicted_noise = torch.lerp(uncond_predicted_noise, predicted_noise, cfg_scale)\n",
    "                alpha = self.alpha[t][:, None, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
    "                beta = self.beta[t][:, None, None, None]\n",
    "                if i > 1:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x)\n",
    "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "        return x\n",
    "\n",
    "def inpaint_image(original_image, generated_image, mask):\n",
    "    \"\"\"\n",
    "    将生成的图像融合到原始图像的指定区域中。\n",
    "    \n",
    "    参数:\n",
    "    original_image (torch.Tensor): 原始图像\n",
    "    generated_image (torch.Tensor): 生成的图像\n",
    "    mask (torch.Tensor): 掩码图像, 1表示需要inpaint的区域, 0表示保留原图\n",
    "    \n",
    "    返回:\n",
    "    torch.Tensor: 输出的合成图像\n",
    "    \"\"\"\n",
    "    # 将三个输入tensor转换到相同的设备上\n",
    "    device = original_image.device\n",
    "    mask = mask.to(device)\n",
    "    generated_image = generated_image.to(device)\n",
    "    \n",
    "    # 使用掩码融合原图和生成的图像\n",
    "    output_image = original_image.clone()\n",
    "    # print(output_image.shape, mask.shape, generated_image.shape)\n",
    "    output_image = output_image * (1 - mask) + generated_image * mask\n",
    "    \n",
    "    return output_image\n",
    "\n",
    "def inpaint_image(original_image, generated_image, mask):\n",
    "    \"\"\"\n",
    "    将生成的图像融合到原始图像的指定区域中。\n",
    "    \n",
    "    参数:\n",
    "    original_image (torch.Tensor): 原始图像\n",
    "    generated_image (torch.Tensor): 生成的图像\n",
    "    mask (torch.Tensor): 掩码图像, 1表示需要inpaint的区域, 0表示保留原图\n",
    "    \n",
    "    返回:\n",
    "    torch.Tensor: 输出的合成图像\n",
    "    \"\"\"\n",
    "    # 将三个输入tensor转换到相同的设备上\n",
    "    device = original_image.device\n",
    "    mask = mask.to(device)\n",
    "    generated_image = generated_image.to(device)\n",
    "    \n",
    "    # 使用掩码融合原图和生成的图像\n",
    "    output_image = original_image.clone()\n",
    "    print(output_image.shape, mask.shape, generated_image.shape)\n",
    "    # print(output_image.shape, mask.shape, generated_image.shape)\n",
    "    reference_image = output_image * mask\n",
    "    output_image = output_image * (1 - mask) + generated_image * mask\n",
    "    generated_image = generated_image * mask\n",
    "    \n",
    "    return output_image, generated_image, reference_image\n",
    "# images_predict_slice, generated_image, reference_image = inpaint_image(images[:,:,:,:], d_images[:,:,:,:], masks[:,:,:,:])\n",
    "\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "args, unknown = parser.parse_known_args()\n",
    "args.run_name = \"DDPM_conditional\"\n",
    "args.batch_size = 2\n",
    "args.image_size = 128#这个参数有什么用，和原始图像的关系是什么\n",
    "# args.dataset_path =  r\"D:\\ASNR-MICCAI-BraTS2023-Local-Synthesis-Challenge-Training\"\n",
    "args.dataset_path =  r\"C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\"\n",
    "args.device = \"cuda\"\n",
    "args.lr = 1e-4\n",
    "args.train = True\n",
    "args.shuffle = False\n",
    "device = 'cuda'\n",
    "# dataloader = get_data_inference(args)\n",
    "model = UNet_conditional_concat_Large().to(device)\n",
    "# model = UNet_conditional_concat().to(device)\n",
    "ckpt = torch.load(\"./models/DDPM_conditional/ema_ckpt.pt\")\n",
    "model.load_state_dict(ckpt)\n",
    "diffusion = Diffusion(img_size=args.image_size, device=device)\n",
    "\n",
    "\n",
    "test_dataloader = get_data_inference(args)\n",
    "pbar_test = tqdm(test_dataloader)\n",
    "for i, (images, cropped_images, masks) in enumerate(pbar_test):\n",
    "    modefied_images = cropped_images\n",
    "    b, _, _, _ = images.shape\n",
    "    # print(modefied_images.shape)\n",
    "    masks = masks.to(torch.float)\n",
    "    # print(masks.shape)\n",
    "    modefied_images = modefied_images.to(device)\n",
    "    d_images = diffusion.sample(model, n=b, labels=modefied_images, masks=masks)\n",
    "    images_predict_slice, generated_image, reference_image = inpaint_image(images[:,:,:,:], d_images[:,:,:,:], masks[:,:,:,:])\n",
    "    images = images.cpu()\n",
    "    d_images_new = images_predict_slice.cpu()\n",
    "    dd = d_images[:,:,:,:].cpu()\n",
    "    cropped_images = modefied_images.cpu()\n",
    "    masks_clone = masks.cpu()\n",
    "\n",
    "\n",
    "    for index in range(b):\n",
    "        z = i * args.batch_size + index\n",
    "        d_img = d_images_new[index, 0, :, :]\n",
    "        # subject_dir = os.path.join(args.dataset_path, f'generated_slice_{z}')\n",
    "        np.savez(os.path.join(args.dataset_path, f'generated_slice_{z}.npz'),\n",
    "                 image = d_img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from utils import *\n",
    "import cv2\n",
    "subject_dir = r'C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\BraTS-GLI-01666-000'\n",
    "print(subject_dir)\n",
    "image_path = glob.glob(os.path.join(subject_dir, '*t1n.nii.gz'))[0]\n",
    "print(image_path)\n",
    "cropped_image_path = glob.glob(os.path.join(subject_dir, '*t1n-voided.nii.gz'))[0]\n",
    "healthy_mask_path = glob.glob(os.path.join(subject_dir, '*healthy.nii.gz'))[0]\n",
    "unhealthy_mask_path = glob.glob(os.path.join(subject_dir, '*unhealthy.nii.gz'))[0]\n",
    "\n",
    "\n",
    "# 加载图像和掩膜\n",
    "image = nib.load(image_path).get_fdata().astype(\n",
    "                np.float32\n",
    "            )\n",
    "healthy_mask = nib.load(healthy_mask_path).get_fdata().astype(\n",
    "                np.float32\n",
    "            )\n",
    "unhealthy_mask = nib.load(unhealthy_mask_path).get_fdata().astype(\n",
    "                np.float32\n",
    "            )\n",
    "cropped_image = nib.load(cropped_image_path).get_fdata().astype(\n",
    "                np.float32\n",
    "            )\n",
    "# image = image_preprocess(image)\n",
    "mask_affine = nib.load(image_path).affine\n",
    "ref_img = nib.Nifti1Image(image, mask_affine)\n",
    "nib.save(ref_img, 'example_ref.nii.gz')\n",
    "\n",
    "# print(image.shape)\n",
    "# cropped_image = image_preprocess(cropped_image)\n",
    "voided_img = nib.Nifti1Image(cropped_image, mask_affine)\n",
    "# nib.save(voided_img, 'example_voided.nii.gz')\n",
    "\n",
    "nonzero_coords = np.nonzero(healthy_mask)\n",
    "center_x = (np.min(nonzero_coords[0]) + np.max(nonzero_coords[0])) // 2\n",
    "center_y = (np.min(nonzero_coords[1]) + np.max(nonzero_coords[1])) // 2\n",
    "center_z = (np.min(nonzero_coords[2]) + np.max(nonzero_coords[2])) // 2\n",
    "image_shape = [240,240,155]\n",
    "# 计算裁剪区域的边界\n",
    "img_size = 128\n",
    "crop_x1 = max(center_x - int(img_size/2), 0)\n",
    "crop_x2 = min(center_x + int(img_size/2), image_shape[0])\n",
    "crop_y1 = max(center_y - int(img_size/2), 0)\n",
    "crop_y2 = min(center_y + int(img_size/2), image_shape[1])\n",
    "# crop_z1 = max(center_z - 48, 0)\n",
    "# crop_z2 = min(center_z + 48, image_shape[2])\n",
    "crop_z1 = np.min(nonzero_coords[2])\n",
    "crop_z2 = np.max(nonzero_coords[2])\n",
    "\n",
    "# # 如果裁剪区域小于 96x96x96,则在另一边扩展\n",
    "crop_size_x = crop_x2 - crop_x1\n",
    "crop_size_y = crop_y2 - crop_y1\n",
    "crop_size_z = crop_z2 - crop_z1\n",
    "# #保存几何坐标信息\n",
    "geometric_list = [crop_x1, crop_x2, crop_y1, crop_y2, crop_z1, crop_z2]\n",
    "\n",
    "if crop_size_x < img_size:\n",
    "    if center_x - int(img_size/2) < 0:\n",
    "        crop_x1 = 0\n",
    "        crop_x2 = img_size\n",
    "    else:\n",
    "        crop_x1 = image_shape[0] - int(img_size)\n",
    "        crop_x2 = image_shape[0]\n",
    "\n",
    "if crop_size_y < img_size:\n",
    "    if center_y - int(img_size/2) < 0:\n",
    "        crop_y1 = 0\n",
    "        crop_y2 = img_size\n",
    "    else:\n",
    "        crop_y1 = image_shape[1] - int(img_size)\n",
    "        crop_y2 = image_shape[1]\n",
    "\n",
    "np_org = cropped_image\n",
    "np_org_clipped = np.percentile(np_org, [0.5, 99.5])\n",
    "start = np.min(np_org_clipped)\n",
    "end = np.max(np_org_clipped)\n",
    "width = end - start\n",
    "print(start, end, width)\n",
    "\n",
    "# 保存每个slice的数据\n",
    "for z in range(44):\n",
    "    subject_slice_path = 'C:/Users/DELL/Desktop/DDPM/ddpm_brats/DDPM_brain/test_data/generated_slice_' + str(z) + '.npz'\n",
    "    with np.load(subject_slice_path) as data:\n",
    "            # np_org = np.asarray(org_nifti.dataobj)\n",
    "            \n",
    "\n",
    "            np_redef = data['image']\n",
    "            \n",
    "\n",
    "            # normalize between ...\n",
    "\n",
    "            clipped_image = np.clip(np_redef, 0, 1)\n",
    "            \n",
    "            # print()\n",
    "            norm_img = (clipped_image - clipped_image.min()) / (\n",
    "                clipped_image.max() - clipped_image.min()\n",
    "            ) * width + start\n",
    "            # kernel_size = (3, 3)  # 高斯核大小\n",
    "            # sigma = 0  # 高斯核标准差,0则自动计算\n",
    "            # norm_img = cv2.GaussianBlur(norm_img, kernel_size, sigma)\n",
    "            # d_image = data['image']\n",
    "            \n",
    "    image[crop_x1:crop_x2, crop_y1:crop_y2, crop_z1+z] = norm_img\n",
    "    # image[:,:, crop_z1+z] = norm_img\n",
    "\n",
    "    # slice_adjacency_image = adjacency_image[:, :, :, z]\n",
    "\n",
    "    # slice_unhealthy_mask = unhealthy_mask[:, :, z]\n",
    "img = nib.Nifti1Image(image, mask_affine)\n",
    "\n",
    "nib.save(img, 'example.nii.gz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inpainting.challenge_metrics_2023 import generate_metrics\n",
    "result_img = nib.load(r'C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\example.nii.gz')\n",
    "result = torch.Tensor(result_img.get_fdata()).unsqueeze(0)\n",
    "\n",
    "# Healthy mask (evaluation volume)\n",
    "mask_path = r'C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\BraTS-GLI-01666-000\\BraTS-GLI-01666-000-mask-healthy.nii.gz'\n",
    "mask_img = nib.load(mask_path)\n",
    "mask_healthy = torch.Tensor(mask_img.get_fdata()).bool().unsqueeze(0)\n",
    "mask_affine = nib.load(mask_path).affine\n",
    "\n",
    "# Reference (ground truth)\n",
    "t1n_path = r'C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\BraTS-GLI-01666-000\\BraTS-GLI-01666-000-t1n.nii.gz'\n",
    "t1n_img = nib.load(t1n_path)\n",
    "t1n = torch.Tensor(t1n_img.get_fdata()).unsqueeze(0)\n",
    "\n",
    "# Normalization Tensor (on what basis shall be normalized? On the model input!)\n",
    "t1n_voided_path = r'C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\BraTS-GLI-01666-000\\BraTS-GLI-01666-000-t1n-voided.nii.gz'\n",
    "t1n_voided_img = nib.load(t1n_voided_path)\n",
    "t1n_voided = torch.Tensor(t1n_voided_img.get_fdata()).unsqueeze(0)\n",
    "\n",
    "def inpaint_image(original_image, generated_image, mask):\n",
    "    \"\"\"\n",
    "    将生成的图像融合到原始图像的指定区域中。\n",
    "    \n",
    "    参数:\n",
    "    original_image (torch.Tensor): 原始图像\n",
    "    generated_image (torch.Tensor): 生成的图像\n",
    "    mask (torch.Tensor): 掩码图像, 1表示需要inpaint的区域, 0表示保留原图\n",
    "    \n",
    "    返回:\n",
    "    torch.Tensor: 输出的合成图像\n",
    "    \"\"\"\n",
    "    # 将三个输入tensor转换到相同的设备上\n",
    "    \n",
    "    # 使用掩码融合原图和生成的图像\n",
    "    output_image = original_image\n",
    "    # print(output_image.shape, mask.shape, generated_image.shape)\n",
    "    # print(output_image.shape, mask.shape, generated_image.shape)\n",
    "    reference_image = output_image * mask\n",
    "    output_image = output_image * (1 - mask) + generated_image * mask\n",
    "    generated_image = generated_image * mask\n",
    "    \n",
    "    return output_image, generated_image, reference_image\n",
    "images_predict_slice, generated_image, reference_image = inpaint_image(t1n_img.get_fdata(), result_img.get_fdata(), mask_img.get_fdata())\n",
    "img = nib.Nifti1Image(images_predict_slice, mask_affine)\n",
    "\n",
    "nib.save(img, 'example.nii.gz')\n",
    "\n",
    "# # Compute metrics\n",
    "metrics_dict = generate_metrics( #expected Tensor dimension: 1 x 255 x 255 x \n",
    "    prediction=torch.Tensor(images_predict_slice).unsqueeze(0),\n",
    "    target=t1n,\n",
    "    mask=mask_healthy,\n",
    "    normalization_tensor= t1n_voided #former: t1n * ~mask_healthy\n",
    "    )\n",
    "\n",
    "print(metrics_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
