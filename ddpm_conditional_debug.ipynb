{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from utils import *\n",
    "from modules import UNet_conditional, EMA, UNet_conditional_concat, UNet_conditional_fully_concat, UNet_conditional_fully_add, UNet_conditional_concat_with_mask, UNet_conditional_concat_with_mask_v2\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", level=logging.INFO, datefmt=\"%I:%M:%S\")\n",
    "\"\"\"\n",
    "需要修改的地方：\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=240, device=\"cuda\"):\n",
    "        self.noise_steps = noise_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "\n",
    "        self.beta = self.prepare_noise_schedule().to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.device = device\n",
    "\n",
    "    def prepare_noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
    "\n",
    "    def noise_images(self, x, t):\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
    "        Ɛ = torch.randn_like(x)\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * Ɛ, Ɛ\n",
    "\n",
    "    def _timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
    "\n",
    "    def sample(self, model, n, labels, masks, cfg_scale=0):\n",
    "        logging.info(f\"Sampling {n} new images....\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn((n, 1, self.img_size, self.img_size)).to(self.device)\n",
    "            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
    "                t = (torch.ones(n) * i).long().to(self.device)\n",
    "                # predicted_noise = model(x, t, labels)\n",
    "                predicted_noise = model(x, t, labels, masks)\n",
    "                if cfg_scale > 0:\n",
    "                    uncond_predicted_noise = model(x, t, None)\n",
    "                    predicted_noise = torch.lerp(uncond_predicted_noise, predicted_noise, cfg_scale)\n",
    "                alpha = self.alpha[t][:, None, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
    "                beta = self.beta[t][:, None, None, None]\n",
    "                if i > 1:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x)\n",
    "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "        return x\n",
    "\n",
    "def inpaint_image(original_image, generated_image, mask):\n",
    "    \"\"\"\n",
    "    将生成的图像融合到原始图像的指定区域中。\n",
    "    \n",
    "    参数:\n",
    "    original_image (torch.Tensor): 原始图像\n",
    "    generated_image (torch.Tensor): 生成的图像\n",
    "    mask (torch.Tensor): 掩码图像, 1表示需要inpaint的区域, 0表示保留原图\n",
    "    \n",
    "    返回:\n",
    "    torch.Tensor: 输出的合成图像\n",
    "    \"\"\"\n",
    "    # 将三个输入tensor转换到相同的设备上\n",
    "    device = original_image.device\n",
    "    mask = mask.to(device)\n",
    "    generated_image = generated_image.to(device)\n",
    "    \n",
    "    # 使用掩码融合原图和生成的图像\n",
    "    output_image = original_image.clone()\n",
    "    # print(output_image.shape, mask.shape, generated_image.shape)\n",
    "    output_image = output_image * (1 - mask) + generated_image * mask\n",
    "    \n",
    "    return output_image\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "args, unknown = parser.parse_known_args()\n",
    "args.run_name = \"DDPM_conditional\"\n",
    "args.batch_size = 10\n",
    "args.image_size = 96#这个参数有什么用，和原始图像的关系是什么\n",
    "# args.dataset_path =  r\"D:\\ASNR-MICCAI-BraTS2023-Local-Synthesis-Challenge-Training\"\n",
    "args.dataset_path =  r\"C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\"\n",
    "args.device = \"cuda\"\n",
    "args.lr = 3e-4\n",
    "args.train = True\n",
    "args.shuffle = False\n",
    "device = 'cuda'\n",
    "dataloader = get_data(args)\n",
    "model = UNet_conditional_concat_with_mask_v2().to(device)\n",
    "# model = UNet_conditional_concat().to(device)\n",
    "ckpt = torch.load(\"./models/DDPM_conditional/ema_ckpt.pt\")\n",
    "model.load_state_dict(ckpt)\n",
    "diffusion = Diffusion(img_size=args.image_size, device=device)\n",
    "pbar = tqdm(dataloader)\n",
    "images, cropped_images, masks = next(iter(pbar))\n",
    "modefied_images = cropped_images\n",
    "b, _, _, _ = images.shape\n",
    "d_images = diffusion.sample(model, n=b, labels=modefied_images, masks=masks)\n",
    "\n",
    "# d_images = diffusion.sample(model, n=b, labels=modefied_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inpaint_image(original_image, generated_image, mask):\n",
    "    \"\"\"\n",
    "    将生成的图像融合到原始图像的指定区域中。\n",
    "    \n",
    "    参数:\n",
    "    original_image (torch.Tensor): 原始图像\n",
    "    generated_image (torch.Tensor): 生成的图像\n",
    "    mask (torch.Tensor): 掩码图像, 1表示需要inpaint的区域, 0表示保留原图\n",
    "    \n",
    "    返回:\n",
    "    torch.Tensor: 输出的合成图像\n",
    "    \"\"\"\n",
    "    # 将三个输入tensor转换到相同的设备上\n",
    "    device = original_image.device\n",
    "    mask = mask.to(device)\n",
    "    generated_image = generated_image.to(device)\n",
    "    \n",
    "    # 使用掩码融合原图和生成的图像\n",
    "    output_image = original_image.clone()\n",
    "    print(output_image.shape, mask.shape, generated_image.shape)\n",
    "    # print(output_image.shape, mask.shape, generated_image.shape)\n",
    "    reference_image = output_image * mask\n",
    "    output_image = output_image * (1 - mask) + generated_image * mask\n",
    "    generated_image = generated_image * mask\n",
    "    \n",
    "    return output_image, generated_image, reference_image\n",
    "images_predict_slice, generated_image, reference_image = inpaint_image(images[:,:,:,:], d_images[:,:,:,:], masks[:,:,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# 选择要显示的图像索引\n",
    "index = 8\n",
    "images = images.cpu()\n",
    "d_images_new = images_predict_slice.cpu()\n",
    "dd = d_images[:,:,:,:].cpu()\n",
    "cropped_images = modefied_images.cpu()\n",
    "g_images = generated_image.cpu()\n",
    "r_images = reference_image.cpu()\n",
    "masks_clone = masks.cpu()\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "# 获取选中的图像\n",
    "img = images[index, 0, :, :]\n",
    "d_img = d_images_new[index, 0, :, :]\n",
    "c_img = cropped_images[index, 0, :, :]\n",
    "g_img = g_images[index, 0, :, :]\n",
    "r_img = r_images[index, 0, :, :]\n",
    "dd_img = dd[index, 0, :, :]\n",
    "mask = masks_clone[index, 0, :, :]\n",
    "# print(g_img)\n",
    "# print(img[10,:])\n",
    "if torch.any(g_img != 0):\n",
    "    print(\"g_img 中包含非0元素\")\n",
    "else:\n",
    "    print(\"g_img 中全是0\")\n",
    "non_zero_elements = g_img[g_img != 0]\n",
    "# print(non_zero_elements)\n",
    "# 使用 matplotlib 显示图像\n",
    "plt.figure(figsize=(8, 8))\n",
    "axes[0,0].imshow(img, cmap='gray')\n",
    "axes[0,0].set_title('Ground-truth')\n",
    "axes[0,1].imshow(c_img, cmap='gray')\n",
    "axes[0,1].set_title('Cropped guidance')\n",
    "axes[1,0].imshow(dd_img, cmap='gray')\n",
    "axes[1,0].set_title('DDPM genarated')\n",
    "axes[1,1].imshow(d_img, cmap='gray')\n",
    "axes[1,1].set_title('Final infilled image')\n",
    "plt.show()\n",
    "mse = nn.MSELoss()\n",
    "# loss = mse(reference_image, masks_clone)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from utils import *\n",
    "# from utils import get_data_inference\n",
    "from modules import UNet_conditional, EMA, UNet_conditional_concat, UNet_conditional_fully_concat, UNet_conditional_fully_add\n",
    "from modules import UNet_conditional_concat_with_mask, UNet_conditional_concat_with_mask_v2, UNet_conditional_concat_Large\n",
    "from modules import UNet_conditional_concat_XLarge, UNet_conditional_concat_with_mask_GAM\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from utils import _structural_similarity_index, _peak_signal_noise_ratio, _mean_squared_error\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", level=logging.INFO, datefmt=\"%I:%M:%S\")\n",
    "\"\"\"\n",
    "需要修改的地方：\n",
    "\n",
    "\"\"\"\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "sigma = 1.0  # 调整此值以控制平滑程度\n",
    "\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=240, device=\"cuda\"):\n",
    "        self.noise_steps = noise_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "\n",
    "        self.beta = self.prepare_noise_schedule().to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.device = device\n",
    "\n",
    "    def prepare_noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
    "\n",
    "    def noise_images(self, x, t):\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
    "        Ɛ = torch.randn_like(x)\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * Ɛ, Ɛ\n",
    "\n",
    "    def _timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
    "\n",
    "    def sample(self, model, n, labels, masks, cfg_scale=0):\n",
    "        logging.info(f\"Sampling {n} new images....\")\n",
    "        model.eval()\n",
    "        noise_img_list = []\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn((n, 1, self.img_size, self.img_size)).to(self.device) + 0.5*labels\n",
    "            # x = labels+masks\n",
    "            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
    "                t = (torch.ones(n) * i).long().to(self.device)\n",
    "                # predicted_noise = model(x, t, labels)\n",
    "                predicted_noise = model(x, t, labels, masks)\n",
    "\n",
    "                alpha = self.alpha[t][:, None, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
    "                beta = self.beta[t][:, None, None, None]\n",
    "                if i > 1:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x)\n",
    "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "\n",
    "                if i % 100 == 0 and i > 200:\n",
    "                    noise_img_list.append(x.detach().cpu())\n",
    "                if i % 40 ==0 and i <= 200:\n",
    "                    noise_img_list.append(x.detach().cpu())\n",
    "\n",
    "        return x, noise_img_list\n",
    "\n",
    "\n",
    "def inpaint_image(original_image, generated_image, mask):\n",
    "    \"\"\"\n",
    "    将生成的图像融合到原始图像的指定区域中。\n",
    "    \n",
    "    参数:\n",
    "    original_image (torch.Tensor): 原始图像\n",
    "    generated_image (torch.Tensor): 生成的图像\n",
    "    mask (torch.Tensor): 掩码图像, 1表示需要inpaint的区域, 0表示保留原图\n",
    "    \n",
    "    返回:\n",
    "    torch.Tensor: 输出的合成图像\n",
    "    \"\"\"\n",
    "    # 将三个输入tensor转换到相同的设备上\n",
    "    device = original_image.device\n",
    "    mask = mask.to(device)\n",
    "    generated_image = generated_image.to(device)\n",
    "    \n",
    "    # 使用掩码融合原图和生成的图像\n",
    "    output_image = original_image.clone()\n",
    "    # print(output_image.shape, mask.shape, generated_image.shape)\n",
    "    # print(output_image.shape, mask.shape, generated_image.shape)\n",
    "    reference_image = output_image * mask\n",
    "    output_image = original_image * (1 - mask) + generated_image * mask\n",
    "    generated_image = generated_image * mask\n",
    "    \n",
    "    return output_image, generated_image, reference_image\n",
    "# images_predict_slice, generated_image, reference_image = inpaint_image(images[:,:,:,:], d_images[:,:,:,:], masks[:,:,:,:])\n",
    "\n",
    "\n",
    "import argparse\n",
    "import re\n",
    "parser = argparse.ArgumentParser()\n",
    "args, unknown = parser.parse_known_args()\n",
    "args.run_name = \"DDPM_conditional\"\n",
    "args.batch_size = 2\n",
    "args.image_size = 96#这个参数有什么用，和原始图像的关系是什么\n",
    "# args.dataset_path =  r\"D:\\ASNR-MICCAI-BraTS2023-Local-Synthesis-Challenge-Training\"\n",
    "args.dataset_path =  r\"C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\test_data_1\"\n",
    "args.generated_data_path = r\"C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\generated_data\"\n",
    "args.device = \"cuda\"\n",
    "args.lr = 1e-4\n",
    "args.train = True\n",
    "args.shuffle = False\n",
    "device = 'cuda'\n",
    "# dataloader = get_data_inference(args)\n",
    "# model = UNet_conditional_concat_Large().to(device)\n",
    "model = UNet_conditional_concat_with_mask_GAM().to(device)\n",
    "ckpt = torch.load(\"./models/DDPM_conditional/204_ema_ckpt.pt\")\n",
    "model.load_state_dict(ckpt)\n",
    "diffusion = Diffusion(img_size=args.image_size, device=device)\n",
    "\n",
    "\n",
    "test_dataloader = get_data_inference(args)\n",
    "print(len(test_dataloader))\n",
    "pbar_test = tqdm(test_dataloader)\n",
    "for i, (images, cropped_images, masks, path) in enumerate(pbar_test):\n",
    "    modefied_images = cropped_images\n",
    "    b, _, _, _ = cropped_images.shape\n",
    "    # print(b)\n",
    "    # print(image_without_healthy.shape)\n",
    "    masks = masks.to(torch.float)\n",
    "    # print(masks.shape)\n",
    "    modefied_images = modefied_images.to(device)\n",
    "    d_images, noise_img_list = diffusion.sample(model, n=b, labels=modefied_images, masks=masks)\n",
    "    images_predict_slice, generated_image, reference_image = inpaint_image(images[:,:,:,:], d_images[:,:,:,:], masks[:,:,:,:])\n",
    "    img = images.cpu()[:,0,:,:]\n",
    "    d_images_new = images_predict_slice.cpu()\n",
    "    dd_img = d_images.cpu()[:, 0, :, :]\n",
    "    mask_img = masks.cpu()[:, 0, :, :]\n",
    "    ref_images_new = reference_image.cpu()\n",
    "    # dd = d_images[:,:,:,:].cpu()\n",
    "    # # cropped_images = modefied_images.cpu()\n",
    "    # # masks_clone = masks.cpu()\n",
    "    # plt.imshow(img[1, :,:] - img[0, :,:], cmap='gray')\n",
    "    # plt.show()\n",
    "\n",
    "    for index in range(b):\n",
    "        # z = i * args.batch_size + index\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "        dd_img_new = dd_img[index, :, :]\n",
    "        d_img = d_images_new[index, 0, :, :]\n",
    "        ref_img = ref_images_new[index, 0, :, :]\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        axes[0,0].imshow(d_img, cmap='gray')\n",
    "        axes[0,1].imshow(dd_img_new, cmap='gray')\n",
    "        axes[1,0].imshow(mask_img[index,:,:], cmap='gray')\n",
    "        axes[1,1].imshow(img[index, :,:], cmap='gray')\n",
    "\n",
    "        plt.show()\n",
    "        orgin_path = path[index]\n",
    "        \n",
    "        filename = os.path.split(orgin_path)[-1]\n",
    "        # print(orgin_path, filename)\n",
    "        \n",
    "        match = re.search(r'96_slice_(\\d+)\\.npz', filename)\n",
    "        number = match.group(1)\n",
    "        print(number)\n",
    "        subject_name = os.path.basename(os.path.dirname(orgin_path))\n",
    "        generated_img_save_folder = os.path.join(args.generated_data_path, subject_name)\n",
    "        os.makedirs(os.path.join(args.generated_data_path, subject_name), exist_ok=True)\n",
    "        generated_img_save_path = os.path.join(generated_img_save_folder, f'generated_slice_{number}.npz')\n",
    "\n",
    "        np.savez(generated_img_save_path,\n",
    "                 image = d_img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in noise_img_list:\n",
    "    # print(x.shape)\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(6, 6))\n",
    "    axes[0].imshow(x[0,0,:,:], cmap='gray')\n",
    "    axes[1].imshow(x[1,0,:,:], cmap='gray')\n",
    "    plt.show()\n",
    "    # images_predict_slice, generated_image, reference_image = inpaint_image(images[:,:,:,:], x[:,:,:,:], masks[:,:,:,:])\n",
    "    # plt.imshow(x[1,0,:,:], cmap='gray')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from utils import *\n",
    "import cv2\n",
    "subject_dir = r'C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\test_data\\BraTS-GLI-01666-000'\n",
    "print(subject_dir)\n",
    "image_path = glob.glob(os.path.join(subject_dir, '*t1n.nii.gz'))[0]\n",
    "print(image_path)\n",
    "cropped_image_path = glob.glob(os.path.join(subject_dir, '*t1n-voided.nii.gz'))[0]\n",
    "healthy_mask_path = glob.glob(os.path.join(subject_dir, '*healthy.nii.gz'))[0]\n",
    "unhealthy_mask_path = glob.glob(os.path.join(subject_dir, '*unhealthy.nii.gz'))[0]\n",
    "\n",
    "\n",
    "# 加载图像和掩膜\n",
    "image = nib.load(image_path).get_fdata().astype(\n",
    "                np.float32\n",
    "            )\n",
    "healthy_mask = nib.load(healthy_mask_path).get_fdata().astype(\n",
    "                np.float32\n",
    "            )\n",
    "unhealthy_mask = nib.load(unhealthy_mask_path).get_fdata().astype(\n",
    "                np.float32\n",
    "            )\n",
    "cropped_image = nib.load(cropped_image_path).get_fdata().astype(\n",
    "                np.float32\n",
    "            )\n",
    "# image = image_preprocess(image)\n",
    "mask_affine = nib.load(image_path).affine\n",
    "ref_img = nib.Nifti1Image(image, mask_affine)\n",
    "nib.save(ref_img, 'example_ref.nii.gz')\n",
    "\n",
    "# print(image.shape)\n",
    "# cropped_image = image_preprocess(cropped_image)\n",
    "voided_img = nib.Nifti1Image(cropped_image, mask_affine)\n",
    "# nib.save(voided_img, 'example_voided.nii.gz')\n",
    "\n",
    "nonzero_coords = np.nonzero(healthy_mask)\n",
    "center_x = (np.min(nonzero_coords[0]) + np.max(nonzero_coords[0])) // 2\n",
    "center_y = (np.min(nonzero_coords[1]) + np.max(nonzero_coords[1])) // 2\n",
    "center_z = (np.min(nonzero_coords[2]) + np.max(nonzero_coords[2])) // 2\n",
    "image_shape = [240,240,155]\n",
    "# 计算裁剪区域的边界\n",
    "img_size = 96\n",
    "crop_x1 = max(center_x - int(img_size/2), 0)\n",
    "crop_x2 = min(center_x + int(img_size/2), image_shape[0])\n",
    "crop_y1 = max(center_y - int(img_size/2), 0)\n",
    "crop_y2 = min(center_y + int(img_size/2), image_shape[1])\n",
    "# crop_z1 = max(center_z - 48, 0)\n",
    "# crop_z2 = min(center_z + 48, image_shape[2])\n",
    "crop_z1 = np.min(nonzero_coords[2])\n",
    "crop_z2 = np.max(nonzero_coords[2])\n",
    "\n",
    "# # 如果裁剪区域小于 96x96x96,则在另一边扩展\n",
    "crop_size_x = crop_x2 - crop_x1\n",
    "crop_size_y = crop_y2 - crop_y1\n",
    "crop_size_z = crop_z2 - crop_z1\n",
    "# #保存几何坐标信息\n",
    "geometric_list = [crop_x1, crop_x2, crop_y1, crop_y2, crop_z1, crop_z2]\n",
    "\n",
    "if crop_size_x < img_size:\n",
    "    if center_x - int(img_size/2) < 0:\n",
    "        crop_x1 = 0\n",
    "        crop_x2 = img_size\n",
    "    else:\n",
    "        crop_x1 = image_shape[0] - int(img_size)\n",
    "        crop_x2 = image_shape[0]\n",
    "\n",
    "if crop_size_y < img_size:\n",
    "    if center_y - int(img_size/2) < 0:\n",
    "        crop_y1 = 0\n",
    "        crop_y2 = img_size\n",
    "    else:\n",
    "        crop_y1 = image_shape[1] - int(img_size)\n",
    "        crop_y2 = image_shape[1]\n",
    "\n",
    "np_org = cropped_image\n",
    "np_org_clipped = np.percentile(np_org, [0.5, 99.5])\n",
    "start = np.min(np_org_clipped)\n",
    "end = np.max(np_org_clipped)\n",
    "width = end - start\n",
    "print(start, end, width)\n",
    "# slice_paths = glob.glob(os.path.join(subject_, 'generated_slice_*.npz'))\n",
    "# 保存每个slice的数据\n",
    "for z in range(45):\n",
    "    subject_slice_path = 'C:/Users/DELL/Desktop/DDPM/ddpm_brats/DDPM_brain/test_data/generated_data/BraTS-GLI-01666-000/generated_slice_' + str(z) + '.npz'\n",
    "    with np.load(subject_slice_path) as data:\n",
    "            # np_org = np.asarray(org_nifti.dataobj)\n",
    "            \n",
    "\n",
    "            np_redef = data['image']\n",
    "            \n",
    "\n",
    "            # normalize between ...\n",
    "\n",
    "            clipped_image = np.clip(np_redef, 0, 1)\n",
    "            \n",
    "            # print()\n",
    "            norm_img = (clipped_image - clipped_image.min()) / (\n",
    "                clipped_image.max() - clipped_image.min()\n",
    "            ) * width + start\n",
    "            # kernel_size = (3, 3)  # 高斯核大小\n",
    "            # sigma = 0  # 高斯核标准差,0则自动计算\n",
    "            # norm_img = cv2.GaussianBlur(norm_img, kernel_size, sigma)\n",
    "            # d_image = data['image']\n",
    "            \n",
    "    image[crop_x1:crop_x2, crop_y1:crop_y2, crop_z1+z] = norm_img\n",
    "    # image[:,:, crop_z1+z] = norm_img\n",
    "\n",
    "    # slice_adjacency_image = adjacency_image[:, :, :, z]\n",
    "\n",
    "    # slice_unhealthy_mask = unhealthy_mask[:, :, z]\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "sigma = 1.0  # 调整此值以控制平滑程度\n",
    "img_smoothed = image.copy()\n",
    "for d in range(image.shape[2]):\n",
    "    img_smoothed[:, :, d] = gaussian_filter(image[:, :, d], sigma=sigma)\n",
    "\n",
    "img_smoothed_v2 = img_smoothed.copy()\n",
    "for d in range(image.shape[0]):\n",
    "    img_smoothed_v2[d, :, :] = gaussian_filter(img_smoothed[d, :, :], sigma=sigma)\n",
    "\n",
    "img_smoothed_v3 = img_smoothed_v2.copy()\n",
    "for d in range(image.shape[1]):\n",
    "    img_smoothed_v3[:, d, :] = gaussian_filter(img_smoothed_v2[:, d, :], sigma=sigma)\n",
    "\n",
    "img = nib.Nifti1Image(image, mask_affine)\n",
    "\n",
    "nib.save(img, 'example.nii.gz')\n",
    "\n",
    "img_smoothed = nib.Nifti1Image(img_smoothed, mask_affine)\n",
    "\n",
    "nib.save(img_smoothed, 'example_smoothed.nii.gz')\n",
    "\n",
    "img_smoothed_v2 = nib.Nifti1Image(img_smoothed_v2, mask_affine)\n",
    "\n",
    "nib.save(img_smoothed_v2, 'example_smoothed_v2.nii.gz')\n",
    "\n",
    "img_smoothed_v3 = nib.Nifti1Image(img_smoothed_v3, mask_affine)\n",
    "\n",
    "nib.save(img_smoothed_v3, 'example_smoothed_v3.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\test_data\\BraTS-GLI-01610-000\n",
      "0.0 688.358277282715 688.358277282715\n",
      "C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\test_data\\BraTS-GLI-01657-000\n",
      "0.0 833.0 833.0\n",
      "C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\test_data\\BraTS-GLI-01658-000\n",
      "0.0 1447.0 1447.0\n",
      "C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\test_data\\BraTS-GLI-01659-000\n",
      "0.0 1795.0 1795.0\n",
      "C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\test_data\\BraTS-GLI-01660-000\n",
      "0.0 1699.0 1699.0\n",
      "C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\test_data\\BraTS-GLI-01661-000\n",
      "0.0 1705.0 1705.0\n",
      "C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\test_data\\BraTS-GLI-01662-000\n",
      "0.0 2876.0 2876.0\n",
      "C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\test_data\\BraTS-GLI-01663-000\n",
      "0.0 881.0 881.0\n",
      "C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\test_data\\BraTS-GLI-01664-000\n",
      "0.0 908.0 908.0\n",
      "C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\test_data\\BraTS-GLI-01665-000\n",
      "0.0 874.0 874.0\n",
      "C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\test_data\\BraTS-GLI-01666-000\n",
      "0.0 1914.0 1914.0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from utils import *\n",
    "import cv2\n",
    "\n",
    "for subject_dir in glob.glob(r'C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\test_data\\*'):\n",
    "    subject_name = os.path.split(subject_dir)[1]\n",
    "    print(subject_dir)\n",
    "    image_path = glob.glob(os.path.join(subject_dir, '*t1n.nii.gz'))[0]\n",
    "    # print(image_path)\n",
    "    cropped_image_path = glob.glob(os.path.join(subject_dir, '*t1n-voided.nii.gz'))[0]\n",
    "    healthy_mask_path = glob.glob(os.path.join(subject_dir, '*healthy.nii.gz'))[0]\n",
    "    unhealthy_mask_path = glob.glob(os.path.join(subject_dir, '*unhealthy.nii.gz'))[0]\n",
    "\n",
    "\n",
    "    # 加载图像和掩膜\n",
    "    image = nib.load(image_path).get_fdata().astype(\n",
    "                    np.float32\n",
    "                )\n",
    "    healthy_mask = nib.load(healthy_mask_path).get_fdata().astype(\n",
    "                    np.float32\n",
    "                )\n",
    "    unhealthy_mask = nib.load(unhealthy_mask_path).get_fdata().astype(\n",
    "                    np.float32\n",
    "                )\n",
    "    cropped_image = nib.load(cropped_image_path).get_fdata().astype(\n",
    "                    np.float32\n",
    "                )\n",
    "    # image = image_preprocess(image)\n",
    "    mask_affine = nib.load(image_path).affine\n",
    "    ref_img = nib.Nifti1Image(image, mask_affine)\n",
    "    # nib.save(ref_img, 'example_ref.nii.gz')\n",
    "\n",
    "    # print(image.shape)\n",
    "    # cropped_image = image_preprocess(cropped_image)\n",
    "    voided_img = nib.Nifti1Image(cropped_image, mask_affine)\n",
    "    # nib.save(voided_img, 'example_voided.nii.gz')\n",
    "\n",
    "    nonzero_coords = np.nonzero(healthy_mask)\n",
    "    center_x = (np.min(nonzero_coords[0]) + np.max(nonzero_coords[0])) // 2\n",
    "    center_y = (np.min(nonzero_coords[1]) + np.max(nonzero_coords[1])) // 2\n",
    "    center_z = (np.min(nonzero_coords[2]) + np.max(nonzero_coords[2])) // 2\n",
    "    image_shape = [240,240,155]\n",
    "    # 计算裁剪区域的边界\n",
    "    img_size = 96\n",
    "    crop_x1 = max(center_x - int(img_size/2), 0)\n",
    "    crop_x2 = min(center_x + int(img_size/2), image_shape[0])\n",
    "    crop_y1 = max(center_y - int(img_size/2), 0)\n",
    "    crop_y2 = min(center_y + int(img_size/2), image_shape[1])\n",
    "    # crop_z1 = max(center_z - 48, 0)\n",
    "    # crop_z2 = min(center_z + 48, image_shape[2])\n",
    "    crop_z1 = np.min(nonzero_coords[2])\n",
    "    crop_z2 = np.max(nonzero_coords[2])\n",
    "\n",
    "    # # 如果裁剪区域小于 96x96x96,则在另一边扩展\n",
    "    crop_size_x = crop_x2 - crop_x1\n",
    "    crop_size_y = crop_y2 - crop_y1\n",
    "    crop_size_z = crop_z2 - crop_z1\n",
    "    # #保存几何坐标信息\n",
    "    geometric_list = [crop_x1, crop_x2, crop_y1, crop_y2, crop_z1, crop_z2]\n",
    "\n",
    "    if crop_size_x < img_size:\n",
    "        if center_x - int(img_size/2) < 0:\n",
    "            crop_x1 = 0\n",
    "            crop_x2 = img_size\n",
    "        else:\n",
    "            crop_x1 = image_shape[0] - int(img_size)\n",
    "            crop_x2 = image_shape[0]\n",
    "\n",
    "    if crop_size_y < img_size:\n",
    "        if center_y - int(img_size/2) < 0:\n",
    "            crop_y1 = 0\n",
    "            crop_y2 = img_size\n",
    "        else:\n",
    "            crop_y1 = image_shape[1] - int(img_size)\n",
    "            crop_y2 = image_shape[1]\n",
    "\n",
    "    np_org = cropped_image\n",
    "    np_org_clipped = np.percentile(np_org, [0.5, 99.5])\n",
    "    start = np.min(np_org_clipped)\n",
    "    end = np.max(np_org_clipped)\n",
    "    width = end - start\n",
    "    print(start, end, width)\n",
    "    slice_paths = glob.glob(os.path.join(r'C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain/test_data/generated_data/', subject_name,'generated_slice_*.npz'))\n",
    "    # print(os.path.join(r'C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain/test_data/generated_data/', subject_name,'generated_slice_*.npz'))\n",
    "    # print(len(slice_paths))\n",
    "    # 保存每个slice的数据\n",
    "    for z in range(len(slice_paths)):\n",
    "        subject_slice_path = os.path.join(r'C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain/test_data/generated_data/', subject_name,'generated_slice_' + str(z) + '.npz')\n",
    "        with np.load(subject_slice_path) as data:\n",
    "                np_redef = data['image']\n",
    "            \n",
    "\n",
    "                # normalize between ...\n",
    "\n",
    "                clipped_image = np.clip(np_redef, 0, 1)\n",
    "                \n",
    "                # print()\n",
    "                norm_img = (clipped_image - clipped_image.min()) / (\n",
    "                    clipped_image.max() - clipped_image.min()\n",
    "                ) * width + start\n",
    "                # print('clipped_image.max()',clipped_image.max(), clipped_image.min())\n",
    "                # print('norm_img.max()',norm_img.max(), norm_img.min())\n",
    "                # print('origin_image.max()',np_redef.max(), np_redef.min())\n",
    "        # print(crop_z1, z, crop_z1+z)\n",
    "        image[crop_x1:crop_x2, crop_y1:crop_y2, crop_z1+z] = norm_img\n",
    "        \n",
    "    import numpy as np\n",
    "    from scipy.ndimage import gaussian_filter\n",
    "    sigma = 1.0  # 调整此值以控制平滑程度\n",
    "    img_smoothed = image.copy()\n",
    "    for d in range(image.shape[2]):\n",
    "        img_smoothed[:, :, d] = gaussian_filter(image[:, :, d], sigma=sigma)\n",
    "\n",
    "    img_smoothed_v2 = img_smoothed.copy()\n",
    "    for d in range(image.shape[0]):\n",
    "        img_smoothed_v2[d, :, :] = gaussian_filter(img_smoothed[d, :, :], sigma=sigma)\n",
    "\n",
    "    img_smoothed_v3 = img_smoothed_v2.copy()\n",
    "    for d in range(image.shape[1]):\n",
    "        img_smoothed_v3[:, d, :] = gaussian_filter(img_smoothed_v2[:, d, :], sigma=sigma)\n",
    "\n",
    "    img = nib.Nifti1Image(image, mask_affine)\n",
    "\n",
    "    nib.save(img, os.path.join(r'C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain/test_data/generated_data/', subject_name,'result.nii.gz'))\n",
    "\n",
    "    img_smoothed = nib.Nifti1Image(img_smoothed, mask_affine)\n",
    "\n",
    "    # nib.save(img_smoothed, 'example_smoothed.nii.gz')\n",
    "\n",
    "    img_smoothed_v2 = nib.Nifti1Image(img_smoothed_v2, mask_affine)\n",
    "\n",
    "    # nib.save(img_smoothed_v2, 'example_smoothed_v2.nii.gz')\n",
    "\n",
    "    img_smoothed_v3 = nib.Nifti1Image(img_smoothed_v3, mask_affine)\n",
    "\n",
    "    nib.save(img_smoothed_v3, os.path.join(r'C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain/test_data/generated_data/', subject_name,'result_smoothed_v3.nii.gz'))\n",
    "    file_path = os.path.join(r'C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain/test_data/generated_data/', subject_name,'result_smoothed_v3.nii.gz')\n",
    "\n",
    "    # 加载 NIfTI 文件\n",
    "    img = nib.load(file_path)\n",
    "    data = img.get_fdata()\n",
    "\n",
    "    # 修改数据\n",
    "    data[data < 50] = 0\n",
    "\n",
    "    # 保存修改后的数据\n",
    "    new_img = nib.Nifti1Image(data, img.affine, img.header)\n",
    "    nib.save(new_img, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 文件路径\n",
    "file_path = r\"C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\generated_data\\BraTS-GLI-01659-000\\result_smoothed_v3.nii.gz\"\n",
    "\n",
    "# 加载 NIfTI 文件\n",
    "img = nib.load(file_path)\n",
    "data = img.get_fdata()\n",
    "\n",
    "# 修改数据\n",
    "data[data < 50] = 0\n",
    "\n",
    "# 保存修改后的数据\n",
    "new_img = nib.Nifti1Image(data, img.affine, img.header)\n",
    "nib.save(new_img, file_path)\n",
    "\n",
    "#要么先做平滑后做normalize\n",
    "#要么normalize后做平滑之后再加一个卡阈值，一旦mask包含背景，平滑会导致背景有值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inpainting.challenge_metrics_2023 import generate_metrics\n",
    "import nibabel as nib\n",
    "import torch\n",
    "\n",
    "result_img = nib.load(r'C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\example.nii.gz')\n",
    "result = torch.Tensor(result_img.get_fdata()).unsqueeze(0)\n",
    "\n",
    "# Healthy mask (evaluation volume)\n",
    "mask_path = r'C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\test_data\\BraTS-GLI-01666-000\\BraTS-GLI-01666-000-mask-healthy.nii.gz'\n",
    "mask_img = nib.load(mask_path)\n",
    "mask_healthy = torch.Tensor(mask_img.get_fdata()).bool().unsqueeze(0)\n",
    "mask_affine = nib.load(mask_path).affine\n",
    "\n",
    "# Reference (ground truth)\n",
    "t1n_path = r'C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\test_data\\BraTS-GLI-01666-000\\BraTS-GLI-01666-000-t1n.nii.gz'\n",
    "t1n_img = nib.load(t1n_path)\n",
    "t1n = torch.Tensor(t1n_img.get_fdata()).unsqueeze(0)\n",
    "\n",
    "# Normalization Tensor (on what basis shall be normalized? On the model input!)\n",
    "t1n_voided_path = r'C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\test_data\\BraTS-GLI-01666-000\\BraTS-GLI-01666-000-t1n-voided.nii.gz'\n",
    "t1n_voided_img = nib.load(t1n_voided_path)\n",
    "t1n_voided = torch.Tensor(t1n_voided_img.get_fdata()).unsqueeze(0)\n",
    "\n",
    "# # Compute metrics\n",
    "metrics_dict = generate_metrics( #expected Tensor dimension: 1 x 255 x 255 x \n",
    "    prediction=result,\n",
    "    target=t1n,\n",
    "    mask=mask_healthy,\n",
    "    normalization_tensor= t1n_voided #former: t1n * ~mask_healthy\n",
    "    )\n",
    "\n",
    "print(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BraTS-GLI-01610-000 0.7639491558074951 10.56435489654541 0.0873989388346672\n",
      "BraTS-GLI-01657-000 0.5881217122077942 14.475207328796387 0.03568447008728981\n",
      "BraTS-GLI-01658-000 0.6736892461776733 14.809887886047363 0.03303780406713486\n",
      "BraTS-GLI-01659-000 0.7168177366256714 20.010009765625 0.008948862552642822\n",
      "BraTS-GLI-01660-000 0.8512413501739502 13.571722984313965 0.04199332371354103\n",
      "BraTS-GLI-01661-000 0.6454342603683472 13.226490020751953 0.047571949660778046\n",
      "BraTS-GLI-01662-000 0.4823773503303528 7.932794570922852 0.16096094250679016\n",
      "BraTS-GLI-01663-000 0.7906925678253174 13.756220817565918 0.042109277099370956\n",
      "BraTS-GLI-01664-000 0.8452531695365906 16.904422760009766 0.02039659395813942\n",
      "BraTS-GLI-01665-000 0.967793881893158 17.548715591430664 0.016946502029895782\n",
      "BraTS-GLI-01666-000 0.5539344549179077 11.373947143554688 0.07287947088479996\n",
      "0.716300444169478\n",
      "14.01579761505127\n",
      "0.0516298304904591\n"
     ]
    }
   ],
   "source": [
    "from inpainting.challenge_metrics_2023 import generate_metrics\n",
    "import nibabel as nib\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "ssim = 0\n",
    "psnr = 0\n",
    "mse = 0\n",
    "counter = 0\n",
    "for subject_dir in glob.glob(r'C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\test_data\\*'):\n",
    "    counter += 1\n",
    "    subject_name = os.path.split(subject_dir)[1]\n",
    "    # print(subject_name)\n",
    "    result_img = nib.load(os.path.join(r'C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain/test_data/generated_data/', subject_name, 'result_smoothed_v3.nii.gz'))\n",
    "    result = torch.Tensor(result_img.get_fdata()).unsqueeze(0)\n",
    "\n",
    "    # Healthy mask (evaluation volume)\n",
    "    mask_path = os.path.join(r'C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\test_data/', subject_name, subject_name + '-mask-healthy.nii.gz')\n",
    "    mask_img = nib.load(mask_path)\n",
    "    mask_healthy = torch.Tensor(mask_img.get_fdata()).bool().unsqueeze(0)\n",
    "    mask_affine = nib.load(mask_path).affine\n",
    "\n",
    "    # Reference (ground truth)\n",
    "    t1n_path = os.path.join(r'C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\test_data/', subject_name, subject_name + '-t1n.nii.gz')\n",
    "    t1n_img = nib.load(t1n_path)\n",
    "    t1n = torch.Tensor(t1n_img.get_fdata()).unsqueeze(0)\n",
    "\n",
    "    # Normalization Tensor (on what basis shall be normalized? On the model input!)\n",
    "    t1n_voided_path = os.path.join(r'C:\\Users\\DELL\\Desktop\\DDPM\\ddpm_brats\\DDPM_brain\\test_data\\test_data/', subject_name, subject_name + '-t1n-voided.nii.gz')\n",
    "    t1n_voided_img = nib.load(t1n_voided_path)\n",
    "    t1n_voided = torch.Tensor(t1n_voided_img.get_fdata()).unsqueeze(0)\n",
    "\n",
    "    # # Compute metrics\n",
    "    metrics_dict = generate_metrics( #expected Tensor dimension: 1 x 255 x 255 x \n",
    "        prediction=result,\n",
    "        target=t1n,\n",
    "        mask=mask_healthy,\n",
    "        normalization_tensor= t1n_voided #former: t1n * ~mask_healthy\n",
    "        )\n",
    "    ssim += metrics_dict['ssim']\n",
    "    psnr += metrics_dict['psnr']\n",
    "    mse += metrics_dict['mse']\n",
    "    print(subject_name, metrics_dict['ssim'], metrics_dict['psnr'], metrics_dict['mse'])\n",
    "print(ssim/counter)\n",
    "print(psnr/counter)\n",
    "print(mse/counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试修改切片能不能生成\n",
    "#看下生成的和原图差多少\n",
    "# 看下原始数据最大最小值的差距\n",
    "# 生成更多数据，或者加入额外的数据集\n",
    "# 尝试多模态数据\n",
    "# 可以加一个分类，告诉模型有没有超出边界"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
